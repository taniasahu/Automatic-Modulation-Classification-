import numpy as np
import scipy.io as sio
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, ReLU, AveragePooling1D, LSTM, Dense, Dropout, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
import matplotlib.pyplot as plt

# Load dataset generated by MATLAB
data = sio.loadmat(r"C:\Users\tania\Downloads\AMC_dataset.mat")
X = data['data']  # Shape: (55000, 1024, 2) for I/Q data
y = data['labels'].flatten() - 1  # Convert to 0-based indexing 
split = data['split'].flatten()

# Split dataset based on the split array (70/15/15)
X_train = X[split == 0]
y_train = y[split == 0]
X_val = X[split == 1]
y_val = y[split == 1]
X_test = X[split == 2]
y_test = y[split == 2]

# Define the CLDNN model
# Input Layer 
input_layer = Input(shape=(1024, 2)) 

# Convolutional blocks (6x [bn, relu, conv, bn, relu, conv, avgpool])
conv_out = input_layer
for _ in range(6):
    conv1 = Conv1D(64, kernel_size=3, padding='same')(conv_out if _ == 0 else conv2)
    bn1 = BatchNormalization()(conv1)
    relu1 = ReLU()(bn1)
    conv2 = Conv1D(64, kernel_size=3, padding='same')(relu1)
    bn2 = BatchNormalization()(conv2)
    relu2 = ReLU()(bn2)
    conv_out = AveragePooling1D(pool_size=2)(relu2)

# LSTM layer for temporal dependencies
lstm_out = LSTM(256, return_sequences=False)(conv_out) 

# Fully connected layers
flatten = Flatten()(lstm_out)
dense1 = Dense(512, activation='relu')(flatten)  
bn_dense = BatchNormalization()(dense1)  # Batch normalization
dropout = Dropout(0.5)(bn_dense)  
output = Dense(11, activation='softmax')(dropout)  

# Create model
model = Model(inputs=input_layer, outputs=output)

# Compile model
model.compile(optimizer=Adam(learning_rate=0.0003), 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Model summary
model.summary()

# Callbacks for learning rate reduction and early stopping
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=0.00001) 
early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)  

# Train model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val),
                    epochs=150, batch_size=32, verbose=1,
                    callbacks=[reduce_lr, early_stop])

# Evaluate on test set
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test accuracy: {test_accuracy*100:.2f}%")

# Save Model 
model.save('cldnn_model.keras') 
